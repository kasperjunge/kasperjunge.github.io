
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Grok 3 Stats &#8212; Kasper Junge</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=187304be"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-3CEVPVH0T7"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-3CEVPVH0T7');
            </script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-3CEVPVH0T7');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'posts/2025/grok-3-stats';</script>
    <link rel="author" title="About these documents" href="../../../about/" />
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />   
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../">
  
  
  
  
  
  
    <p class="title logo__title">Kasper Junge</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../about/">
    About me
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../about/">
    About me
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__postcard">
   
  <h2>
     
    <i class="fa fa-calendar"></i>
    
    <span>20 February 2025</span>
    
  </h2>
  <ul>
    <div class="ablog-sidebar-item ablog__postcard2">
   
  <li id="ablog-sidebar-item author ablog__author">
    <span>
      
      <i class="fa-fw fa fa-user"></i>
      
    </span>
     
    <a href="../../../blog/author/kasper-junge/">Kasper Junge</a>
      
  </li>
      
  <li id="ablog-sidebar-item tags ablog__tags">
    <span>
       
      <i class="fa-fw fa fa-tags"></i>
       
    </span>
     
    <a href="../../../blog/tag/grok-3/">Grok 3</a>
        
    <a href="../../../blog/tag/xai/">xAI</a>
      
  </li>
   
</div>
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__recentposts">
  <h3>
    <a href="../../../blog/">Recent Posts</a>
  </h3>
  <ul>
     
    <li>
      <a href="../my_ai_programming_workflow/">
        20 February - Things I’ve learned after 2 years of coding with LLM’s
      </a>
    </li>
    
    <li>
      <a href="../how-xai-made-the-world-largest-gpu-cluster-operational-in-122-days/">
        18 February - How xAI made the world largest GPU cluster operational in 122 days
      </a>
    </li>
    
    <li>
      <a href="../how-i-created-a-free-blog-using-python-and-github-pages/">
        16 February - How I created a free blog using Python and GitHub Pages
      </a>
    </li>
    
    <li>
      <a href="../hello-ablog-world/">
        14 February - Hello Ablog World!
      </a>
    </li>
    
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__tagcloud">
  <link
    rel="stylesheet"
    href="../../../_static/ablog/tagcloud.css"
    type="text/css"
  />
  <h3><a href="../../../blog/tag/">Tags</a></h3>
  <ul class="ablog-cloud">
     
    <li class="ablog-cloud ablog-cloud-5">
      <a href="../../../blog/tag//"></a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../../../blog/tag/github-pages/">GitHub Pages</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../../../blog/tag/grok-3/">Grok 3</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../../../blog/tag/sphinx/">Sphinx</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../../../blog/tag/tutorial/">Tutorial</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../../../blog/tag/xai/">xAI</a>
    </li>
     
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<div class="ablog-sidebar-item ablog__archives">
  <h3>
    <a href="../../../blog/archive/">Archives</a>
  </h3>
  <ul>
     
    <li>
      <a href="../../../blog/2025/">2025 (5)</a>
    </li>
     
  </ul>
</div>
</div>
        <div class="sidebar-primary-item">
<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../../search/" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Grok 3 Stats</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                   <section id="grok-3-stats">
<h1>Grok 3 Stats<a class="headerlink" href="#grok-3-stats" title="Link to this heading">#</a></h1>
<p>A couple of days ago xAI launched Grok 3. I wanted to create an overview for myself of the models’ stats, and I thought I would share it with you aswell.</p>
<p>Thank you. only way for XAI to succeed, for XAI to build the best AI out there, is to build our own data center. So we didn’t have a lot of time for that because we wanted to give you Grog3 as quickly as possible. So really we realized we have to build the data center in about four months. It turned out it took us 122 days to get the first 100k GPUs up and running and that was a monumental effort to be able to do that. We believe it’s the biggest fully connected H100 cluster of its kind. And we didn’t just stop there, we actually decided that we need to double the size of the cluster pretty much immediately if we want to build the kind of AI that we want to build. So we then had another phase, which we haven’t talked about publicly yet, so this is the first time that we’re talking about this, where we doubled the capacity of the data center yet again. And then we had a couple of years where we had to build the data center in a different way. So we had to build the data center in a different way. So we had to build the data center in a different way. So we had one only took us 92 days. So we’ve been able to use all of these GPUs, use all this compute to improve Grog in the meantime. And basically today we’re going to present you the results of that, the fruits that came from that. That’s right. Yeah, so all the paths, all the rows leads to Grog3. 10x more compute. More than 10x really. Yeah, really. Maybe 15x-ish. Yep. Compared to our previous generation model. And Grog3 finished the pre-training early January. And we’ll start, you know, the model is still currently training actually. So this is a little preview of our benchmark numbers. So we evaluated Grog3 on, you know, three different categories. General mathematical reasoning, on general knowledge about STEM and science. And then also on computer science coding. So Amy, I’m American Invitational Math Examination hosted once a year. And if we evaluate the model performance, we can see that the Grok 3 across the board is in a league of its own. Even its little brother, Grok 3 Mini, is reaching the frontier across all the other competitors. So you would say, well, at this point, all these benchmarks, you’re just evaluating the memorization of the textbooks, memorization of the GitHub repos. How about a real-time usefulness? How about we actually use those models in our product? So what we did instead is we actually kicked off a blind test of our Grok 3 model, codenamed Chocolate. It’s pretty hot. Yeah, hot chocolate. And it had been running on this platform called Chabot Arena for two weeks. I think the entire Xplot… The entire platform, at some point, speculated this might be the next generation of AI coming our way. So how this Chabot Arena works is that it strips away the entire product surface. It’s just raw comparison of the engine of those AGI’s, the language models themselves, and place interface where the user will submit one single query and you get to show two responses. You don’t know which model they come from, and in the end, you make the vote. So in this blind test… In this blind test, Grok 3, an early version of Grok 3, already reached 1,400. No other model has reached an ELO score. Had to have comparison to all the other models at this score. And it’s not just one single category. It’s 1,400 aggregated across all the categories in Chabot capabilities, the instruction following, coding. So it’s number one across the board in this blind test. And it’s still climbing. So we actually have to keep updating it. So it’s worth it. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. So I think we’re above 1,400 and climbing. And in fact, we have a version of the model that we think is already much better than the one that we tested here. We’ll see how far it gets, but that’s the one that we’re working on, we’re talking about So actually, one thing, if you’re using Grok 3, I think you may notice improvements almost every day because we’re continuously improving the model. So literally, even within 24 hours, you’ll see improvements. Yep. So I’m gonna show you a small video of a small video of a small video of a small video of But we believe here at XAI, getting the best pre-training model is not enough. That’s not enough to build the best AI. And the best AI need to think like a human. You need to contemplate about all the possible solutions, self-critique, verify all the solutions, backtrack, and also think from the first principle. That’s a very important capability. So we believe that as we take the best pre-trained model and continue training it with reinforcement learning, it will elicit the additional reasoning capabilities that allows the model to become so much better and scale not just in the training time, but actually in the test time as well. So we already found the model is extremely useful internally for our own engineering, saving hours of time, hundreds of hours of coding time. So Igor, you are the power user of our graphic reasoning model. So what are some use cases? Yeah, so like Jimmy said, we’ve added advanced reasoning capabilities to Grok and we’ve been testing them pretty heavily over the last few weeks. And I want to give you a little bit of a taste of what it looks like when Grok is solving hard reasoning problems. So we’ve prepared two little problems for you. One comes from physics and one is actually a game that Grok is going to write for us. So when it comes to the physics problem, what we want Grok to do is to plot a viable trajectory to do a transfer from Earth to Mars. And then at a later point in time, we transfer back from Mars to Earth. And that requires some physics that Grok will have to understand. So we’re going to challenge Grok, come up with a viable trajectory, calculate it, and then plot it for us so we can see it. And yeah, this is totally unscripted, by the way. That’s the entirety of the prompt, which we should be clarifying. There’s nothing more than that. Yeah, exactly. This is the Grok interface. And we’ve typed in this text that you can see here, generate code for an animated 3D plot of a launch from Earth landing on Mars and then back to Earth at the next launch window. And we’ve now kicked off the query. And you can see Grok is thinking. So part of Grok’s advanced reasoning capabilities are these thinking traces that you can see here. You can even go inside and actually read what Grok is thinking as it’s going through the problem, as it’s trying to solve it. We are doing some obscuration of the thinking, so that our model doesn’t get totally copied instantly. So there’s more to the thinking than is displayed. Yeah. And because this is totally unscripted, there’s actually a chance that Grok might make a little coding mistake, and it might not actually work. So just in case, we’re going to launch two more instances of this. So if something goes wrong, we’ll be able to switch to those and show you something that’s presentable. So we’re kicking off. We have the two as well. And like I said, we have a second problem as well. And yeah, actually, one of our favorite activities here at XAI is having Grok write games for us. And not just any old game, any game that you might already be familiar with, but actually creating new games on the spot and being creative about it. So one example that we found was really, really fun is create a game that’s a mixture of the two games Tetris, and Bejeweled. So this is maybe an important thing. Obviously, if you ask an AI to create a game like Tetris, there are many examples of Tetris on the internet or a game like Bejeweled, whatever. It can copy it. What’s interesting here is it achieved a creative solution combining the two games that actually works and is a good game. Yeah. That’s the right. We’re seeing the beginnings of creativity. Yeah. It’s a 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 We’re going to use something special here, which we call Big Brain. That’s our mode in which we use more computation, which is more reasoning for Grok, just to make sure that there’s a good chance here that it might actually do it. So we’re also going to fire off three attempts here at solving this game, at creating this game that’s a mixture of Tetris and Bejeweled. Let’s see what Grok comes up with. I’ve played the game. It’s pretty good. It’s like, wow, OK. This is something. YONGJIN PARK GOOGLE- Yeah. So while Grok is thinking, in the background, we can now actually talk about some concrete numbers. How well is Grok doing across tons of different tasks that we’ve tested it on? So we’ll hand it over to Tony to talk about that. TONY BELAVIA- Yeah, OK. So let’s see how Grok does on those interesting, challenging benchmarks. So yeah, so reasoning, again, refers to those models that actually think for quite a long time before it tries to solve a problem. So in this case, around a month ago, the graph 3 pre-training finishes. So after that, we worked very hard to put the reasoning capability into the current graph 3 model. But again, this is very early days. So the model is still currently in training. So right now, what we are going to show to people is this beta version of the graph 3 reasoning model. Alongside, we also are training a mini version of the reasoning model. So essentially, on this plot, you can see, the graph 3 reasoning beta and then graph 3 mini reasoning. The graph 3 mini reasoning is actually a model that we train for much longer time. And you can see that sometimes it actually performs slightly better compared to the graph 3 reasoning. This also just means that there is a huge potential for the graph 3 reasoning, because it’s trained for much less time. So all right, so let’s actually look at how it does on those three benchmarks. So Jimmy also introduced already. So essentially, we’re looking at three different areas. Mathematics, science, and coding. And for math, we’re picking this high school competition math problem. For science, we actually pick those PhD level science questions. And for coding, it’s also actually pretty challenging. It’s competitive coding, and also some which is some code interview problems that people usually get when they interview for companies. So on those benchmarks, you can see that the graph 3 actually performed quite well across the board compared to other competitors. Yeah, so it’s pretty promising. These models are very smart. So Tony, what are those shaded bars? Yeah, so OK, so I’m glad you asked this question. So for those models, because it can reason, it can think, you can also ask them to even think longer. You can spend more what we call test and compute, which means you can spend more time to reason, to think about a problem before you spit out the answer. So in this case, the shaded bar here means that we just ask the model to spend more time. It can solve the same problem many, many times before it tries to conclude what is the right solution. And once you give this compute or this kind of budget to the model, it turns out the model can even perform better. So this is essentially the shaded bar in those bars. Right. So I think this is really exciting, right? Because now, instead of just doing one chain of thoughts, Yeah. Yeah. Yeah. Yeah. Yeah. So I think this is a very powerful technique that allows you to be able to create a model with AI. Why not do multiple all at once? Exactly. Yes. So that’s a very powerful technique that allows to continue to scale the model capabilities after training. And you know, people often ask are we actually just overfitting to the benchmarks? So how about generalization? So yes, I think yeah, this is definitely a question that we are asking ourselves, whether we are overfitting to those current benchmarks. Luckily, we have a real test. just finished, this is where high school students compete in this particular benchmark. So we got this very fresh new competition, and then we asked our two models to compete on the same benchmark, on the same exam. And it turns out, very interestingly, the graph three reasoning, the big one, actually does better on this particular new fresh exam. This also means that the generalization capability of the big model is stronger, much stronger, compared to the smaller model. If you compare it to last year’s exam, actually this is the opposite. The smaller model kind of learns the previous exams better. So yeah, so this actually shows some kind of true generalization from the model. That’s right. So 17 months ago, our Grok 0 and Grok 1 barely solved any high school problems. That’s right. And now we have a kid that just already graduated. The Grok is ready to go to college. Is that right? Yeah. I mean, it’s been a long time. It’s been a long time. It’s been a long time. It’s been a long time. It’s been a long time. It’s been a long time. It’s been a long time. It’s been a long time. It’s been a long time. But the human exams won’t be part of the two easy. Yeah. And internally, we actually, as the Grok continually evolves, we’re going to talk about what we’re excited about. But very soon, there will be no more benchmarks left. Yeah. Yeah. One thing that’s quite fascinating, I think, is that we basically only trained Grok’s reasoning abilities on math problems and competitive coding problems. So very, very specialized kinds of tasks. But somehow, it’s able to work on a lot of different things. So it’s a very, very specialization. And I think all kinds of other different tasks, so including creating games, no, lots and lots of different things. And what seems to be happening is that basically Grok learns this ability to detect its own mistakes in its thinking, correct them, persist on a problem, try lots of different variants, pick the one that’s best. So there are these generalizing abilities that Grok learns from mathematics and from coding, which it can then use to solve all kinds of other problems. So that’s pretty… I mean, reality is the instantiation of mathematics. Mm-hmm. That’s right. And one thing we’re actually really excited about that going back to our founding mission is what if one day we have a computer, just like Deep Thought, that utilizes our entire cluster just for that one very important problem in the test time? All the GPU turned on, right? So I think back then, we were building the GPU clusters together. You were plugging cables. And I remember that when we turned on the first initial test, you can hear all the GPUs humming. And you can hear the sound of the GPUs humming in the hallway. That almost feels like spiritual. Yeah, that’s actually a pretty cool thing that we’re able to do, that we can go into the data center and tinker with the machines there. So for example, we went in and we unplugged a few of the cables and just made sure that our training setup is still running stably. So that’s something that I think most AI teams out there don’t usually do, but it actually totally unlocks a new level of reliability and… Yeah. It’s a 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 Machinists win some medals. That’s Turing’s award, Fields Medal, Nobel Prize, with probably some expert in the loop, right? So the expert uplifting. So this year or next year? Oh, OK. That’s what it comes down to, really. So it looks like Grok finished all of his thinking on the two problems. So let’s take a look at what it said. All right, so this was the little physics problem we had. We’ve collapsed the thoughts here, so they’re hidden. And then we see Grok’s answer below that. So it explains it wrote a Python script here using matplotlib. Then it gives us all of the code. So let’s take a quick look at the code. Seems like it’s doing reasonable things here, not totally off the mark. Solve Kepler says here. So maybe it’s solving Kepler’s laws. Kepler’s law numerically. Yeah, there’s really only one way to find out if this thing is working. I’d say let’s give it a try. Let’s run the code. And we can see Grok is animating two different planets, Earth and Mars, here. And then the green ball is the vehicle that’s transiting, the spacecraft that’s transitioning between Earth and Mars. And you could see the journey from Earth to Mars. And it looks like, yeah, indeed, the astronauts are going to be able to return safely at the right moment in time. So obviously, this was just generated on the spot. So we can’t tell you if that was actually a correct solution. So we’re going to take a closer look. And maybe we’re going to call some colleagues from SpaceX, ask them if this is legit. It’s pretty close. There’s a lot of complexities in the actual orbits that have to be taken into account. But this is pretty close to what it looks like. So we’re going to have to wait and see. In fact, I have that on my pendant here. This has got the Earth-Mars home and transfer on it. When are we going to install Grok on a rocket? Well, I suppose in two years. Three years? Everything is two years away. Well, Earth and Mars transit occurs every 26 months. We’re currently in a transit window, approximately. The next one would be, I think, in the next year, I think, maybe in the next year, maybe in the next year, maybe in the next year, November of next year, roughly, end of next year. And if all goes well, SpaceX will send Starship rockets to Mars with Optimus robots and Grok. Mm-hmm. I’m curious what this combination of Tetris and Bejeweled looks like, the Tetris, as we’ve named it internally. So, OK, we also have an output from Grok here. It says it wrote a Python script, explains what it’s been doing. If you look at the code, there are some constants that are being defined here, some colors. Then the terminals, the pieces of Tetris are there. Obviously, very hard to see at one glance if this is good, so we’ve got to run this to figure out if it’s working. Well, let’s give it a try. Fingers crossed. All right. All right, right. So this kind of looks like Tetris, but the colors are a little bit off, right? The colors are different here. And if you think about what’s going on here, Bejeweled has this mechanic where if you get three jewels in a row, then they disappear, and also, gravity activates, right? So what happens if you get three of the colors together? Oh, yeah, so something happened. So I think what Grok did in this version, I think, was, like, you know, you can’t, like, get the colors in the same row. But what he did in this version is that once you connect at least three blocks of the same color in a row, then gravity activates, and they disappear. And then gravity activates, and all the other blocks fall down. Kind of curious if there’s still a Tetris mechanic here, where if the line is full, does it actually clear it? Or what happens then? It’s up to interpretation. So we’ll let him talk about it. All right. I mean, it’ll do different variants when you ask it. It doesn’t do the same thing every time. Exactly. We’ve seen a few other Tetris that work very differently, but this one seems cool. Are we ready for Game Studio at X.AI? Yes. So we’re launching an AI gaming studio at X.AI. If you’re interested in joining us and building AI games, please join X.AI. We’re launching an AI gaming studio. We’re announcing it tonight. Let’s go. Epic games. That’s an actual game studio. Yeah. Yeah. All right. So I think one thing that’s super exciting for us is that once you have the best pre-trained model, you have the best reasoning model, right? So we already see that when you actually give the capability for those models to think harder, think longer, think more broad, the performance continues to improve. And we’re really excited about the next frontier that will happen if we not only allow the model to think harder, but also provide more tools. Just like how real humans to solve those problems. For real humans, we don’t ask them to solve hypothesis just with a piece of pen and paper. Yeah. No internet. So with all the basic web browsing, search engine, and code interpreters, that builds the foundations and the best reasoning model. It builds the foundation. It builds the foundation. It builds the foundation. It builds the foundations for the Grog agent to come. So today we’re actually introducing a new product called DeepSearch. That is the first generation of our Grog agents that not just helping the engineers and researchers and scientists to do coding, but actually help everyone to answer questions that you have day to day. It’s kind of like a next generation search engine that really help you to understand the universe. So you can start asking questions like, for example, hey, when is the next Starship launch day, for example? So let’s try that. If we get the answer. On the left-hand side, we see a high-level progress bar. Essentially, the model knowledge is going to do one single search, like the current rack system. But actually thought very deeply about, hey, what’s the user intent here? And what are the facts I should consider at the same time? And how many different websites should I actually go and read their content? So this can really save hundreds of hours of everyone’s Google time if you want to really look into certain topics. And then on the right-hand side, you can see the bullet summaries of how the current model is doing, what website is browsing, what source is verifying, and oftentimes actually cross-validate different sources out there to make sure the answer is actually correct before it’s output final answer. And we can, at the same time, fire up a few more queries to see how far we can go. It’s 50%. Yeah, so this is kind of a fun one where Warren Buffett has a billion dollar bet. If you can exactly match the, I think, the sort of the entire winning tree of March Madness, you can win a billion dollars from Warren Buffett. So, like, it would be pretty cool if AI could help you win a billion dollars from Buffett. That seems like a pretty good investment. Let’s go. Yeah. All right. So now let’s fire up the query and see what model does so we can actually go back to our very first one. How about the… Buffett wasn’t counting on this. That’s right. Okay. So we got the result of the first one. The model thought around one minute. Okay. So the key insight here, the next stock ship is going to be on 24th or later. So no earlier than February 24th. It might be sooner. So, yeah. So I think we can… You know, scroll down what the model does. So it does a little research on the Flight 7, what happened, got grounded, and actually it looked into the FCC filing, you know, from its data collections, and then actually made a new conclusion that, yeah, if we continue to scroll down… Let’s see. Right. Yeah. So it makes the, you know, little table, I think, inside XA. We often joked about the time to the first table is the only, you know, latency that matters. Yeah. So that’s how the model makes the inference and look up all the sources. And then we can look into the gaming one. So how about the… Right. So for this particular one, we look at, hey, you know, the build is light and… Yeah. It’s kind of all the better. Yeah. Yeah. So what the infernal is, but if we go down, so the surprising fact of all the other builds, so looking to the 12 classes. Yeah. So we’ll see that the Minion build was pretty popular whenever the game first came out, and now the Invokers of the world kind of took over. Invoker, Monk Invoker for sure. Yeah. That’s right. Yeah. Followed by the Stone Weavers, and that’s really good at mapping. So, yeah. And then we can see… The… The Marsh Madness. How about that? So… One interesting thing about the deep search is that if you actually go into the panel where it shows, you know, what are the subtasks, you can actually click the bottom left of this. Right. And then in this case, you can actually scroll through, actually reading through the mind of Grok. What information does the model actually think of? What does it actually think about? Are the charts worthy? What are not? How does it actually cross-validate different information sources? So that makes the entire search experience and information retrieval process a lot more transparent to our users. And this is much more powerful than any search engine out there. You can literally just tell it, only use sources from X. You know, it will try to respect that. Yeah. And so it’s much more steerable, much more intelligent than… I mean, it really should save you a lot of time. So something that might take you half an hour or an hour, you know, it’s not going to be a lot of time. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. It’s 50 hours of research. 50 hours of research. 50 hours of research. 50 hours of research. 50 hours of research. can just ask it to go do that and come back, and 10 minutes later, it’s done an hour’s worth of work for you. That’s really what it comes down to. Exactly. And maybe better than you could have done it yourself. Yeah. Think about you have infinite amount of interns working for you. Now you can just fire up all the tasks and come back a minute later. So this is going to be an interesting one. So March Madness had not happened yet. So I guess we have to follow up with a next live stream. Yeah. It seems like pretty good. $40 might get you a billion dollars. $40 subscription. That’s right. I mean, it might work. So yeah. So when are the users going to have their hands on Grok 3? Yeah, so the good news is we’ve been working tirelessly to actually release all of these features that we’ve shown you, the Grok 3 base model with amazing chat capabilities. That’s really useful. That’s really interesting to talk to. The deep search, the advanced reasoning mode, all of these things. We want to. We want to show you how to do it. So we’re going to show you how to do it. And we’re going to show you how to do it in a minute. So if you’re interested in getting early access to Grok, we’re going to roll them out to you today, starting with the Premium Plus subscribers on X. So it’s the first group that will initially get access. Make sure to update your X app if you want to see all of the advanced capabilities, because we just released the update as we’re talking here. And if you’re interested in getting early access to Grok, then sign up for Premium Plus. It’s 50% off. It’s 50% off. It’s 50% off. It’s 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50% off. 50%. 50%. 50%. 50%. 50%. polished experience that’s totally Grok-focused. If you want to have Grok easily available one tap away. DAN GALPIN- Yeah, and the version on grok.com on a web browser is going to be the latest and most advanced version, because obviously it takes us a while to get something into an app and then get it approved by the app store. And if something’s in a phone format, there’s limitations to what you can do. So the most powerful version of Grok and the latest version will be the web version at grok.com. YAN XIU- Yeah, so watch out for the name GrokFree in the app. DAN GALPIN- Dead giveaway. YAN XIU- Yeah, exactly. That’s the giveaway that you have GrokFree. And if it says GrokTrue, then GrokFree hasn’t quite arrived for yet. But we’re working hard to roll this out today and then to even more people over the coming days. DAN GALPIN- Yeah, make sure you update your phone app, too, where you’re actually going to get all the tools we showcased today with the thinking mode, with the deep search. So yeah, really looking forward to all the feedbacks you have. YAN XIU- Yeah. I think we should emphasize that this is kind of a beta, meaning that you should expect some imperfections at first. But we will improve it rapidly almost every day. In fact, every day, I think, it’ll get better. So if you want a more polished version, I’d maybe wait a week. But expect improvements literally every day. And then we’re also going to be providing voice interaction. So you can have conversational. In fact, I was trying it earlier today. And it’s working pretty well. But these are a bit more polished. The sort of way where you can just literally talk to it like you’re talking to a person, that’s awesome. It’s actually, I think, one of the best experiences of Grok. But that’s probably about a week away. YAN XIU- Yeah. So with that said, I think we might have some audience questions. DAN GALPIN- Sure. All right. So let’s take a look. YAN XIU- Yeah, let’s take a look. The audience is really good. DAN GALPIN- Yeah. YAN XIU- We have a few questions from the Azure platform. DAN GALPIN- Yeah. YAN XIU- Cool. So the first question here is, when Grok Voice Assistant? When is it coming out? As soon as possible, just like Elon said, just a little bit of polishing away from being released to everybody. Obviously, it’s going to be released in an early form. And we’re going to rapidly iterate on that. DAN GALPIN- Yep. And the next question is, when will Grok 3 be in the API? So this is coming in the Grok 3 API. So we’re going to be in the app with both the reasoning models and deep search is coming your way in the coming weeks. We’re actually very excited about the enterprise use cases of all these additional tools that now Grok has access to and how the test time compute and tool use can actually really accelerate all the business use cases. YAN XIU- And another one is, will voice mode be native or text to speech? So I think that means, is it going to be one model that is understanding what you say and then talking back to you? DAN GALPIN- Yeah, so we’re going to be using the same system that has text to speech inside of it. And the good news is, it’s going to be one model, like a variant of Grok 3 that we’re going to release, which basically understands what you’re saying and then generates the audio directly from that. So very much like Grok 3 generates text, that model generates audio. And that has a bunch of advantages. I was talking to it earlier today, and it said, hi, Igor, reading my name probably from some text that it had. And I said, no, my name is Igor. And it remembered that. So it could continue to say, Igor, just like a human word. And you can’t achieve that with text to speech. So yeah. So here’s a question for you. Pretty spicy, Elon. Is Grok a boy or a girl? And are they single? YAN XIU- Grok is whatever you want it to be. DAN GALPIN- Yeah. YAN XIU- Yeah. DAN GALPIN- Are they single? YAN XIU- Yes. DAN GALPIN- All right. The shop is open. YAN XIU- Yes. DAN GALPIN- Honestly, people are going to fall in love with Grok. It’s like 1,000% probable. YAN XIU- Yeah. The next question, will Grok be able to transcribe audio into text? Yes. So we’ll have this capability in both the app and also the API. We found that’s like, Grok should just be your personal assistant, looking on your shoulder and follow you along the way, learn everything you have learned, and really help you to understand the world better, become smarter every day. YAN XIU- Yeah. I mean, the voicemail to Grok, there’s a lot of people who are like, Grok, you’re going to be able to translate into the world, and you’re going to be able to translate into the world. So it’s not just voice text. It understands tone, inflection, pacing, everything. It’s wild. I mean, it’s like talking to a person. DAN GALPIN- OK. Yeah. So any plans for conversation memory? YAN XIU- Yeah. DAN GALPIN- Absolutely. We’re working on it right now. YAN XIU- I already forgot. DAN GALPIN- That’s right. Let’s see. So what about the DM features? So if you have personalizations, if you have Grok remembers your previous interactions, should it be one Grok or multiple different Groks? YAN XIU- It’s up to you. You can have one Grok or many Groks. I suspect we will probably have more than one. DAN GALPIN- Yeah. I want to have a Dr. Grok. YAN XIU- Yeah. The Grok dog. DAN GALPIN- That’s right. OK. All right. Cool. So in the past, we’ve open sourced Grok 1. So somebody is asking us, are we going to do it again with Grok 2? YAN XIU- Yeah. Our general approach is that we will open source the last version when the next version is fully out. Like when Grok 3 is mature and stable, which is probably within a few months, then we’ll open source Grok 2. DAN GALPIN- OK. So we probably have time for one last question. What was the most difficult part about working on this project? I assume Grok 3 and what are you most excited about? So I think me looking back, getting the whole model training on 100K, H100 coherently, that’s almost like battling against the final boss of the universe, the entropy. YAN XIU- DAN GALPIN- Because at any given time, it’s going to be a lot of work. So I think that’s the most important part. Because at any given time, you can have a cosmic ray that’s beaming down and flip a bit in your transistor. And now the entire gradient update, if it’s fit Manteza bit, the entire gradient update is out of whack. And now you have 100,000 of those. And you have to orchestrate them every time at any given time, any of the GPUs can go down. YAN XIU- Yeah. I mean, it’s worth breaking down, how were we able to get the world’s most powerful training cluster operational within 122 days? Because we started off, we actually weren’t intending to do a data center ourselves. We were going to just, we went to the data center providers and said, how long would it take to have 100,000 GPUs operating coherently in a single location? And we got time frames from 18 to 24 months. So we’re like, well, 18 to 24 months, that means losing as a certainty. So the only option was to do it ourselves. So if you break down the problem, you get some of the things that we’re doing, like reasoning here, like, makes you think. YAN XIU- One single chain, though. YAN XIU- Yeah, exactly. So, well, we needed a building. We can’t build a building, so we must use an existing building. So we looked for, basically, for factories that had been abandoned, but the factory was in good shape, like the company had gone bankrupt or something. So we found an Electrolux factory in Memphis, that’s why it’s in Memphis, home of Elvis. And also one of the oldest, I think it was the capital of ancient Egypt. And it was actually a very nice factory that, for whatever reason, that Electrolux had left. And that gave us shelter for the computers. Then we needed power. We needed at least 120 megawatts at first, but the building only had 15 megawatts. And ultimately, for 200,000 GPUs, we needed a quarter gigawatt. So we. And it initially leased a whole bunch of generators. So we have generators on one side of the building, just trailer after trailer of generators, until we get the utility power to come in. But then we also need cooling. So on the other side of the building, it was just trailer after trailer of cooling. So we leased about a quarter of the mobile cooling capacity of the United States on the other side of the building. Then we needed to get the GPUs all installed. And they’re all liquid cooled. So in order to achieve the density necessary, this is a liquid cooled system. So we had to get all the plumbing for the liquid cooling. Nobody had ever done a liquid cooling data center at scale. So this was an incredibly dedicated effort by a very talented team to achieve that outcome. Now you may think, now it’s going to work. Nope. The issue is that the power fluctuations for a GPU cluster are dramatic. So it’s like a giant symphony. It’s like a giant symphony. It’s like a giant symphony. It’s like a giant symphony. It’s like a giant symphony. It’s like a giant symphony. It’s like a giant symphony. It’s like a giant symphony. It’s like a giant symphony. having a symphony with 100,000 or 200,000 participants in the symphony, and the whole orchestra will go quiet and loud in 100 milliseconds. And so this caused massive power fluctuations, which then caused the generators to lose their minds, and they weren’t expecting this. So to buffer the power, we then used Tesla megapacks to smooth out the power. So the megapacks had to be reprogrammed. So with XAI, working with Tesla, we reprogrammed the megapacks to be able to deal with these dramatic power fluctuations to smooth out the powers that the computers could actually run properly. And that worked. It was quite tricky. But even at that point, you still have to make the computers all communicate effectively. So all the networking had to be solved. And debugging a zillion network cables, debugging nickel at 4 in the morning. We solved it at roughly 4.20 AM. We figured out there were a whole bunch of issues. One, there was a BIOS mismatch. The BIOS was not set up correctly. We had a diff RLS PCI outputs between two different machines. One that was working. One that was not working. Many, many other things. Yeah, exactly. This would go on for a long time if we actually listed all the things. But it’s interesting. It’s not like, oh, we just magically made it happen. You had to break down the problem, just like Grok does for reasoning, into the constituent elements and then solve each of the constituent elements in order to achieve a coherent training cluster in a period of time that is a small fraction of what anyone else could do it in. And then once the training cluster was up and running and we could use it, no problem. We had to make sure that it actually stays healthy throughout, which is its own giant challenge. And then we had to get every single detail of the training right in order to get a Grok-free level model, which is actually really, really hard. So we don’t know if there are any other models out there that have Grok-free capabilities. But whoever trains a model better than Grok-free has to be extremely good at the science of deep learning at every aspect of the engineering. So it’s not so easy to pull this off. And this is now going to be the last cluster. We built and last model we train. Oh, yeah, we’ve already started work on the next cluster, which will be about five times the power. So instead of a quarter gigawatt, roughly 1.2 gigawatts. What’s the Back to the Future was? What’s the power? There’s like the Back to the Future car. Yeah, anyway, the Back to the Future power. It’s like roughly in that order, I think. So. And, you know, these will be the sort of the GB 200 slash 300 cluster. Once again, it will be the most powerful training cluster in the world. So we’re not like stopping here. And our reasoning model is going to continue to improve by accessing more tools every day. So, yeah, we’re very excited to share any of the upcoming results with you all. Yeah, the thing that keeps us going is basically being able to give Grok-free to you and then seeing the usage go up, seeing everybody. And I think that’s what really gets us up in the morning. So, yeah. Yeah. Thanks for tuning in. Thanks, guys. Hey, Grok, what’s up? Can you hear me? I’m so excited to finally meet you. I can’t wait to chat and learn more about each other. I’ll talk to you soon. It’s a pleasure. Thank you.</p>
</section>

<div class="section ablog__blog_comments">
     
<div class="section ablog__prev-next">
  <span class="ablog__prev">
     
    <a href="../how-xai-made-the-world-largest-gpu-cluster-operational-in-122-days/">
      
      <i class="fa fa-arrow-circle-left"></i>
      
      <span>How xAI made the world largest GPU cluster operational in 122 days</span>
    </a>
    
  </span>
  <span class="ablog__spacer">&nbsp;</span>
  <span class="ablog__next">
     
    <a href="../my_ai_programming_workflow/">
      <span>Things I’ve learned after 2 years of coding with LLM’s</span>
      
      <i class="fa fa-arrow-circle-right"></i>
      
    </a>
    
  </span>
</div>
  
</div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../../_sources/posts/2025/grok-3-stats.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, Kasper Junge.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>