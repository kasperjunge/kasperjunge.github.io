<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title></title>
    <link rel="stylesheet" href="/_static/theme.css" />
  </head>
  <body>
    <main class="container">
      <div class="post" data-tags="" data-author="Kasper Junge">
      <p>Feb 26, 2025</p>
      </div>
      <h1 id="investigating-the-deepseek-deepgemm-release">Investigating
      the DeepSeek DeepGEMM Release</h1>
      <p>DeepSeek just released <a
      href="https://github.com/deepseek-ai/DeepGEMM">DeepGEMM</a>, as
      the third release in their <a
      href="https://github.com/deepseek-ai/open-infra-index">Open-Sourcing
      5 AI Repos in 5 Days</a> series.</p>
      <p>I'm no LLM infra engineer, so I am probably not the primary
      audience for this release.</p>
      <p>Also, I can't claim that I would fully understand what was
      going on under the hood in DeepGEMM if I read the code. At the
      very least, it would take me quite some time.</p>
      <p>But I'm curious. So I thought I would write a short blog post
      where I share my understanding of what DeepGEMM is and what it can
      be used for.</p>
      <p>In this blog post, I will go through the repo, read the
      comments on the <a
      href="https://news.ycombinator.com/item?id=43179478">Hacker News
      post of the release</a>, and write about what I learn.</p>
      <h2 id="programming-languages-used">Programming Languages
      Used</h2>
      <p>By looking at the DeepGEMM repository on GitHub, the first
      thing I notice is that the code is written in 58.9% CUDA and 41.1%
      Python.</p>
      <p>This tells me that the content of this code likely consists of
      CUDA kernels that optimize some of the calculations executed when
      running LLM training and inference. (It's fine, just call me a
      genius.)</p>
      <p>The fact that almost half of the code is Python could indicate
      that the optimized CUDA kernels are made available as Python
      bindings through a deep learning framework like PyTorch.</p>
      <h2 id="the-readme">The README</h2>
      <p>Surprisingly, the README file contains a lot of information
      about the project.</p>
      <p>It states that DeepGEMM is:</p>
      <blockquote>
      <p><em>a library designed for clean and efficient FP8 General
      Matrix Multiplications (GEMMs) with fine-grained scaling, as
      proposed in DeepSeek-V3.</em></p>
      </blockquote>
      <p>FP8 is an 8-bit floating point format, which I suppose is used
      to represent model weights in order to reduce the memory footprint
      of the model and thus speed up training and inference.</p>
      <p>General Matrix Multiplications (GEMMs) is a term I've never
      heard before. However, after some research, I quickly learned that
      this is basically just the operation of multiplying two matrices
      to produce a third matrix‚Äîan operation heavily used when training
      and running deep learning models.</p>
      <p>Then, it is mentioned that the library provides
      <em>fine-grained scaling</em>. After some research, it seems that
      this has to do with managing numerical underflow/overflow when
      working with such low precision. The reason it is called
      fine-grained is that it does not use a single global scaling
      factor for the entire matrix but instead applies different scaling
      factors to different sections of the matrix to ensure numerical
      stability. So one of the big problems it solves is keeping matrix
      multiplications numerically stable with a precision as low as
      FP8.</p>
      <p>One thing I'm curious about now is whether this is a new
      concept or if it has been around for a while.</p>
      <h2 id="terms-that-i-dont-know-andor-understand">Terms That I
      Don't Know and/or Understand</h2>
      <p>In the README, there were many terms I either didn't know or
      didn't fully understand.</p>
      <p>The first was <a
      href="https://github.com/nvidia/cutlass">CUTLASS</a>, which
      appears to be an abbreviation for <em>CUDA Templates for Linear
      Algebra Subroutines and Solvers</em>. It is a library developed by
      NVIDIA that provides high-performance matrix-multiplication code
      optimized for NVIDIA GPUs. DeepGEMM takes inspiration from CUTLASS
      but is simpler. DeepGEMM seems to differentiate from CUTLASS by
      providing support for FP8 precision.</p>
      <p>The second term I didn't know was <a
      href="https://github.com/NVIDIA/cutlass/tree/main/include/cute">CuTe</a>,
      which, like CUTLASS, is a library that provides CUDA templates.
      (I'm not entirely sure what is meant by templates in a CUDA
      context.)</p>
      <p>The DeepSeek engineers give credit to both the CUTLASS and the
      CuTe library in the README and mention that DeepGEMM took
      inspiration from it.</p>
      <h2 id="hacker-news-comments">Hacker News Comments</h2>
      <p>To be honest, the comments on the Hacker News post of the
      release was way above my knowledge of GPU programming. However, I
      summarized the top 20 points from the comments with ChatGPT and to
      me it was super interesting to read. So here at the end, I will be
      super lazy and just paste the key points from the comments:</p>
      <ol type="1">
      <li>DeepGEMM claims 2x‚Äì2.5x speedup over its own CUTLASS-based
      baseline.</li>
      <li>Uncertainty remains on performance vs. cuBLAS, which is the
      gold standard for NVIDIA GEMMs.</li>
      <li>FFMA SASS interleaving optimization improves FP8 GEMM
      performance by 10%+.</li>
      <li>Yield-bit manipulation allows warp-level parallelism, reducing
      stalls and improving register reuse.</li>
      <li>Similar techniques were used in NVIDIA Maxwell (2015) but
      hadn‚Äôt been applied to FP8 GEMMs.</li>
      <li>Reverse engineering NVIDIA‚Äôs SASS assembly has been a key part
      of these optimizations.</li>
      <li>Some corporations and hedge funds have implemented similar
      optimizations privately.</li>
      <li>Google and other AI firms have also explored undocumented
      NVIDIA optimizations.</li>
      <li>DeepSeek's open-source MIT license approach is appreciated but
      benefits big AI companies more than individual developers.</li>
      <li>FP8 precision may not be sustainable long-term, as it relies
      on the assumption that models are inherently sparse.</li>
      <li>Native microscaling support (MXFP) in Blackwell GPUs may
      replace DeepGEMM‚Äôs scaling tricks.</li>
      <li>NVIDIA‚Äôs CUDA advantage ("moat") may weaken if more AI
      accelerators integrate similar optimizations in hardware.</li>
      <li>DeepGEMM only runs on Hopper GPUs (H100, H800) due to Tensor
      Core dependencies.</li>
      <li>Attempts to run it on consumer GPUs (RTX 5080) failed due to
      shared memory limitations.</li>
      <li>Lowering memory settings in gemm.py may allow it to work on
      lower-end GPUs.</li>
      <li>GPU compiler technology still lags behind‚Äîmanual optimizations
      like this show how much performance is left on the table.</li>
      <li>Future GPUs may reduce the need for low-level software
      optimizations as hardware catches up.</li>
      <li>NVIDIA keeps some GPU features undocumented, leading to
      reverse engineering by researchers and companies.</li>
      <li>AI engineers interested in GPU programming should learn CUDA,
      SIMT, warp scheduling, and memory hierarchies.</li>
      <li>Books recommended for learning GPU programming: Programming
      Massively Parallel Processors (Wen-Mei Hwu) and Advanced GPU
      Assembly Programming (Gareth Thomas).</li>
      </ol>
      <h2 id="conclusion">Conclusion</h2>
      <p>All in all, DeepGEMM seems to be a library that provides CUDA
      kernels that can be accessed via PyTorch and enables numerically
      stable FP8 precision to speed up training and inference of Large
      Language Models.</p>
      <p>If you want to check out the two previous DeepSeek infra
      releases, visit the <a
      href="https://github.com/deepseek-ai/FlashMLA">FlashMLA</a> and <a
      href="https://github.com/deepseek-ai/DeepEP">DeepEP</a> repos.
      üëç</p>
    </main>
  </body>
  </html>


